{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import nltk, re\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib\n",
    "import os\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5023\n"
     ]
    }
   ],
   "source": [
    "json_data=open(\"C:/Users/hshah/OneDrive - Paycor, Inc/Documents/Anly 610/Result_Deduplicated.json\").readlines()\n",
    "import json\n",
    "feeds_read_from_file = []\n",
    "for line in json_data:\n",
    "    feeds_read_from_file.append(json.loads(line))\n",
    "print(len(feeds_read_from_file)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of titles: 5023\n"
     ]
    }
   ],
   "source": [
    "feed_titles = []\n",
    "\n",
    "for feed in feeds_read_from_file:\n",
    "    feed_titles.append(str(feed['title']))\n",
    "\n",
    "print(\"Total number of titles: \" + str(len(feed_titles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_titles(title):\n",
    "    tokens = nltk.word_tokenize(title)\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    filtered_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        token = token.replace(\"'s\", \" \").replace(\"n’t\", \" not\").replace(\"’ve\", \" have\")\n",
    "        token = re.sub(r'[^a-zA-Z0-9 ]', '', token)\n",
    "        if token not in stopwords:\n",
    "            filtered_tokens.append(token.lower())\n",
    "    \n",
    "    lemmas = [lmtzr.lemmatize(t,'v') for t in filtered_tokens]\n",
    "\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clstr_lda(num_topics, stories):\n",
    "    # top words to be identified\n",
    "    n_top_words = 10\n",
    "\n",
    "    tf_vectorizer = CountVectorizer(max_df=100, min_df=2, max_features=500,\n",
    "                                    tokenizer=tokenize_titles, ngram_range=(3,4))\n",
    "\n",
    "    tf = tf_vectorizer.fit_transform(stories)\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics, max_iter=200,\n",
    "                                    learning_method='online', learning_offset=10.,\n",
    "                                    random_state = 1)\n",
    "    lda.fit(tf)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "    import pyLDAvis\n",
    "    import pyLDAvis.sklearn\n",
    "    import matplotlib\n",
    "    import os\n",
    "   \n",
    "    # print top topic words\n",
    "    topics = dict()\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        topics[topic_idx] = [tf_feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" | \".join([tf_feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        pyLDAvis.enable_notebook()\n",
    "        panel = pyLDAvis.sklearn.prepare(lda, tf, tf_vectorizer)\n",
    "        panel\n",
    "    \n",
    "    return topics\n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "ipad pro  | apple watch series | april 1  |  10 million | donate  10 million | donate  10 |  april 1  |  april 1 | oprah donate  | oprah winfrey donate\n",
      "Topic #1:\n",
      "home dark  | iphone  ipad |  home dark |  ipad  |  home dark  | iphone  ipad  | apple watch  |   apple | apple cider vinegar | galaxy s20 ultra\n",
      "Topic #2:\n",
      "   |     |  apple  | app dark sky | weather app dark sky | weather app dark |  499  | new macbook air | app dark sky  | new apple tv\n",
      "Topic #3:\n",
      "apple  new |  apple watch | iphone 9  | release date  | new ipad pro | live update  | call iphone se | apple macbook pro |  update  | apple app store\n",
      "Topic #4:\n",
      " aapl  |  nasdaq  | nasdaq  aapl  |  nasdaq  aapl | nasdaq  aapl | inc  nasdaq | apple inc  nasdaq | inc  nasdaq  | apple inc  | google alert \n",
      "Topic #5:\n",
      "apple iphone 9 |  samsung  |  xiaomi  | case  slim |  free ship | apple  samsung | covid19 screen app | slim stylish apple mobile | stylish apple mobile phone | stylish apple mobile\n",
      "Topic #6:\n",
      "iphone se  | dark sky  |  video  |  iphone se | dark sky weather app | sky weather app | dark sky weather | apple buy dark | buy dark sky | apple buy dark sky\n",
      "Topic #7:\n",
      " 2020  | april 3  | macbook pro  | iphone se 2 | 2020 ipad pro |  april 3 | 2020   | macbook air  |  april 3  |  2020  \n",
      "Topic #8:\n",
      "apple tv  |  apple tv | iphone 11 pro | apple iphone 11 |  new post | new post  |  new post  |  try  | 11 pro max | iphone 11 pro max\n",
      "Topic #9:\n",
      "amazon prime video | logic pro x | iphone 9 release | huawei p40 pro | iphone 9 release date | 9 release date | samsung galaxy s20 | apple imac 215 | 30  cut |  reg \n"
     ]
    }
   ],
   "source": [
    "topics = clstr_lda(10, feed_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
